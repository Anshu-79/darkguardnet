{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up library constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing & Cleaning up database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'dataset.tsv'\n",
    "df = pd.read_csv(datapath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.drop(df[df.category == 'Not Dark Pattern'].index)\n",
    "df = df.drop(df[df.category == 'Forced Action'].index)\n",
    "df = df.drop(df[df.category == 'Sneaking'].index)\n",
    "df = df.drop(df[df.category == 'Obstruction'].index)\n",
    "\n",
    "sns.countplot(df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for val,key in enumerate(np.unique(df.category)):\n",
    "    labels[key] = val\n",
    "\n",
    "class_names = list(labels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED = 'bert-base-cased'\n",
    "BATCHSIZE = 32\n",
    "TRAIN_RATIO = 0.8\n",
    "EVAL_RATIO = 0.9\n",
    "DROPOUT = 0.1\n",
    "NUM_LABELS = len(class_names)\n",
    "MODULE_ROOT = os.getcwd()\n",
    "PROJECT_ROOT = Path(MODULE_ROOT).resolve().parents[1]\n",
    "PRETRAINED_MODEL_DIR = os.path.join(PROJECT_ROOT, \"dark_pattern_spotter\\\\pretrained_models\")\n",
    "PRETRAINED_MODEL = os.path.join(PRETRAINED_MODEL_DIR, os.listdir(PRETRAINED_MODEL_DIR)[-1])\n",
    "EPOCHS = 10\n",
    "LR = 3e-5\n",
    "MODEL_NAME = \"best-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_lens = []\n",
    "\n",
    "for txt in df.text:\n",
    "  tokens = tokenizer.encode(txt, max_length=512)\n",
    "  token_lens.append(len(tokens))\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256])\n",
    "plt.xlabel('Token count')\n",
    "\n",
    "MAXLENGTH = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset consisting of text & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarkPatternDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = [text for text in df['text']]\n",
    "        self.targets = [labels[category] for category in df['category']]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = tokenizer.encode_plus(text,\n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=MAXLENGTH,\n",
    "                                         return_token_type_ids=False,\n",
    "                                         padding='max_length',\n",
    "                                         return_attention_mask=True,\n",
    "                                         truncation=True,\n",
    "                                         return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Dataloaders for training, evaluation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=1-TRAIN_RATIO, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5)\n",
    "\n",
    "\n",
    "def create_data_loader(df, batch_size):\n",
    "\n",
    "  ds = DarkPatternDataset(df)\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "  )\n",
    "\n",
    "train_dataloader = create_data_loader(df_train, BATCHSIZE)\n",
    "val_dataloader = create_data_loader(df_val, BATCHSIZE)\n",
    "test_dataloader = create_data_loader(df_test, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a specialised model for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarkPatternClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=DROPOUT):\n",
    "        super(DarkPatternClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRETRAINED)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, NUM_LABELS)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the model & assigning it to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DarkPatternClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the shape of model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "\n",
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising an `optimizer`, `scheduler` & a `loss_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LR, correct_bias=False)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in tqdm(dataloader):\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        targets = d['targets'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in dataloader:\n",
    "            input_ids = d['input_ids'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            targets = d['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing our pre-trained Dark Pattern Spotter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(PRETRAINED_MODEL)\n",
    "model_dict = model.state_dict() \n",
    "pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_dict}\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-'*10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, device, scheduler, len(df_train))\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, val_dataloader, loss_fn, device, len(df_val))\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}\\n')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        model_path = os.path.join(MODULE_ROOT, f\"models\\\\{MODEL_NAME}.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['train_acc'] = [x.cpu() for x in history['train_acc']]\n",
    "history['val_acc'] = [x.cpu() for x in history['val_acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, _ = eval_model(model, test_dataloader, loss_fn, device, len(df_test))\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A helper function to get more analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader):\n",
    "    model = model.eval()\n",
    "\n",
    "    texts = []\n",
    "    predictions = []\n",
    "    predictions_probs = []\n",
    "    real_vals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in dataloader:\n",
    "            text = d[\"text\"]\n",
    "            input_ids = d['input_ids'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            targets = d['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            texts.extend(text)\n",
    "            predictions.extend(preds)\n",
    "            predictions_probs.extend(outputs)\n",
    "            real_vals.extend(targets)\n",
    "    \n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    predictions_probs = torch.stack(predictions_probs).cpu()\n",
    "    real_vals = torch.stack(real_vals).cpu()\n",
    "\n",
    "    return texts, predictions, predictions_probs, real_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_texts, y_pred, y_pred_probs, y_test = get_predictions(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One might need to remove categories with low frequencies in the dataset\n",
    "# because there might not be any results with said category in the test dataset\n",
    "\n",
    "#class_names.remove(\"Forced Action\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True Category')\n",
    "    plt.xlabel('Predicted category')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running model on raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "someText = \"Some sample text\"\n",
    "encoded = tokenizer.encode_plus(\n",
    "        someText,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAXLENGTH,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt')\n",
    "\n",
    "input_ids = encoded['input_ids'].to(device)\n",
    "attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "output = model(input_ids, attention_mask)\n",
    "print(torch.max(output, dim=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
