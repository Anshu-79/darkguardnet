{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED = 'roberta-large'\n",
    "BATCHSIZE = 32\n",
    "MAXLENGTH = 32\n",
    "SEED = 42\n",
    "TRAIN_RATIO = 0.8\n",
    "EVAL_RATIO = 0.9\n",
    "DROPOUT = 0.1\n",
    "NUM_LABELS = 8\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "EPOCHS = 5\n",
    "LR = 3e-5\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device = torch.device('cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "config = AutoConfig.from_pretrained( 'bert-base-uncased', output_hidden_states=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'dataset.tsv'\n",
    "df = pd.read_csv(datapath, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_text = \"This is some sample text\"\n",
    "bert_input = tokenizer(sample_text, padding='max_length', max_length=MAXLENGTH, truncation = True, return_tensors='pt')\n",
    "\n",
    "print(bert_input)\n",
    "text_passed_in = tokenizer.decode(bert_input.input_ids[0])\n",
    "print(text_passed_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'Not Dark Pattern':0,\n",
    " 'Misdirection':1,\n",
    " 'Forced Action':2,\n",
    " 'Obstruction':3,\n",
    " 'Scarcity':4,\n",
    " 'Sneaking':5,\n",
    " 'Social Proof':6,\n",
    " 'Urgency':7}\n",
    "\n",
    "class DarkPatternDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.labels = torch.tensor([labels[label] for label in df['Pattern Category']])\n",
    "        self.texts = [tokenizer(text, padding='max_length',max_length=MAXLENGTH, truncation=True, return_tensors='pt') for text in df['text']]\n",
    "        #self.texts = [text['input_ids'] for text in self.texts]\n",
    "\n",
    "    def classes(self):\n",
    "        return labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def get_batch_labels(self, idx):\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_labels\n",
    "\n",
    "d = DarkPatternDataset(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(SEED)\n",
    "df_train, df_val, df_test = np.split(\n",
    "    df.sample(frac=1, random_state=SEED),\n",
    "    [int(TRAIN_RATIO*len(df)), int(EVAL_RATIO*len(df))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarkPatternClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout = DROPOUT):\n",
    "        super(DarkPatternClassifier, self).__init__()\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED, config=config)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, NUM_LABELS)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.model(input_ids = input_id, attention_mask = mask, return_dict = False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1884\n",
      "942.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551f2ab656334c8abb1a9045e2e1d5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n",
      "2\n",
      "tensor(1) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(DarkPatternDataset(df_train), batch_size=BATCHSIZE, shuffle=True)\n",
    "print(len(train_dataloader.dataset))\n",
    "print(len(train_dataloader.dataset)/BATCHSIZE)\n",
    "for train_input, train_label in tqdm(train_dataloader):\n",
    "    train_label = train_label.to(device)\n",
    "    mask = train_input['attention_mask'].to(device)\n",
    "    input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "    print(input_id.shape[0])\n",
    "    print(torch.max(mask), torch.min(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = DarkPatternDataset(train_data), DarkPatternDataset(val_data)\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=BATCHSIZE, shuffle=True)\n",
    "    val_dataloader = DataLoader(val, batch_size=BATCHSIZE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    for epoch_num in range(epochs):        \n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            batch_loss = criterion(output, train_label.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                \n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "                f\"\"\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = DarkPatternDataset(test_data)\n",
    "\n",
    "    test_dataloader = DataLoader(test, batch_size=BATCHSIZE)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model):\n",
    "    model_path = join(PROJECT_ROOT, \"models\\\\roberta-large.pth\")\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'classifier.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.6.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'classifier.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DarkPatternClassifier()\n",
    "\n",
    "checkpoint = torch.load(\"D:\\\\Programming\\\\DPBH-Team_Veritas\\\\dark_pattern_binary_classifier\\\\pretrained_models\\\\roberta-large_4.pth\")\n",
    "model_dict = model.state_dict() \n",
    "pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_dict}\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bd74d07738450db353d376c6cecfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     21\u001b[0m mask \u001b[38;5;241m=\u001b[39m train_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m input_id \u001b[38;5;241m=\u001b[39m train_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m criterion(output, train_label\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     27\u001b[0m total_loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[78], line 13\u001b[0m, in \u001b[0;36mDarkPatternClassifier.forward\u001b[1;34m(self, input_id, mask)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_id, mask):\n\u001b[1;32m---> 13\u001b[0m     _, pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     dropout_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n\u001b[0;32m     15\u001b[0m     linear_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(dropout_output)\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1014\u001b[0m     embedding_output,\n\u001b[0;32m   1015\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1024\u001b[0m )\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:232\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    229\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    235\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanwa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, df_test)\n",
    "save(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
